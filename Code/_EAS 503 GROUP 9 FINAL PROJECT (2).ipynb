{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Yi Ting Huang,John Enright, Subhash Madineni (GROUP 9)\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a21df351597e2934052f8fcd5c9a6ece",
     "grade": true,
     "grade_id": "cell-eda95574f90f9367",
     "locked": true,
     "points": 40,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ecd97e2e2c4e6756301b47afee2c8207",
     "grade": false,
     "grade_id": "cell-f341bec8a17a004b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Create as many cells as you need BELOW this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql 'select * from Unemployment where year = 2020 or year =2019;': no such table: Unemployment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1680\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1681\u001b[0;31m             \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1682\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcur\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOperationalError\u001b[0m: no such table: Unemployment",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f1644dba7bb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unemployment.db'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0msql_statement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"select * from Unemployment where year = 2020 or year =2019;\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_sql_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql_statement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mread_sql_query\u001b[0;34m(sql, con, index_col, coerce_float, params, parse_dates, chunksize)\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0mcoerce_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoerce_float\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_dates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m     )\n\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mread_query\u001b[0;34m(self, sql, index_col, coerce_float, params, parse_dates, chunksize)\u001b[0m\n\u001b[1;32m   1725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1726\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_convert_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1727\u001b[0;31m         \u001b[0mcursor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1728\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcol_desc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol_desc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1692\u001b[0m             \u001b[0mex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatabaseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Execution failed on sql '{args[0]}': {exc}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1693\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1695\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDatabaseError\u001b[0m: Execution failed on sql 'select * from Unemployment where year = 2020 or year =2019;': no such table: Unemployment"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from sqlite3 import Error\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import json\n",
    "from urllib.request import urlopen\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl_dates\n",
    "from datetime import datetime, timedelta\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "\n",
    "def create_connection(db_file, delete_db=False):\n",
    "    import os\n",
    "    if delete_db and os.path.exists(db_file):\n",
    "        os.remove(db_file)\n",
    "\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_file)\n",
    "        conn.execute(\"PRAGMA foreign_keys = 1\")\n",
    "    except Error as e:\n",
    "        print(e)\n",
    "\n",
    "    return conn\n",
    "##Create Tables\n",
    "conn = create_connection('Unemployment.db')\n",
    "sql_statement = \"select * from Unemployment where year = 2020 or year =2019;\"\n",
    "df = pd.read_sql_query(sql_statement, conn)\n",
    "display(df)\n",
    "\n",
    "conn = create_connection('Unemployment.db')\n",
    "sql_statement = \"select * from income;\"\n",
    "df1 = pd.read_sql_query(sql_statement, conn)\n",
    "display(df)\n",
    "\n",
    "#Join Tables\n",
    "df1['Name of County']= df1['Name of County'].astype(str)\n",
    "list_lower =[]\n",
    "for county in df1['Name of County']:\n",
    "    if 'County' in county.split():\n",
    "        list_lower.append( county.replace(' County',''))\n",
    "len(list_lower)\n",
    "df1 = df1.rename(columns={'':'Median_Income'})\n",
    "df2 = pd.DataFrame()\n",
    "df2['County'] = list_lower\n",
    "df2['Median_Income'] = df1['Median_Income'].tolist()[:62]\n",
    "\n",
    "main_df = df.merge(df2,on=['County'])\n",
    "main_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_lower =[]\n",
    "for county in df1['Name of County']:\n",
    "    if 'County' in county.split():\n",
    "        list_lower.append( county.replace(' County',''))\n",
    "len(list_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2020 = main_df[main_df['Year']==2020]\n",
    "df_2019 = main_df[main_df['Year']==2019]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.rename(columns={'':'Median_Income'})\n",
    "df2 = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with urlopen('https://cugir.library.cornell.edu/download/file/cugir-007865-geojson.json') as response:\n",
    "    counties = json.load(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(counties['features'])) :\n",
    "    print(counties['features'][i]['properties']['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc = main_df['Median_Income'].tolist()\n",
    "for i in range(len(inc)) :\n",
    "    inc[i]=inc[i].replace(',','')\n",
    "main_df['Median_Income'] = inc\n",
    "main_df['Median_Income'] = main_df['Median_Income'].astype(int)\n",
    "main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df['County'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coun = main_df['County'].unique().tolist()\n",
    "mean_amount = []\n",
    "for c in coun :\n",
    "    mean_amount.append(df_2020[df_2020['County']==c]['Benefit_Amount'].mean())\n",
    "dd = pd.DataFrame()\n",
    "dd['County']= coun\n",
    "dd['Benefit_Amount']=mean_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2020[df_2020['County']=='Albany']['Benefit_Amount'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Vizulizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean beneficiary Amounts 2019 vs 2020\n",
    "\n",
    "dfpr1 = main_df[main_df['Year']==2019].groupby(['Month'])['Benefit_Amount'].mean()\n",
    "vals = []\n",
    "for i in list(range(1,13)) :\n",
    "    vals.append(dfpr1[i])\n",
    "vals\n",
    "fig1 = go.Figure()\n",
    "fig1.update_layout(title_text = 'Mean Beneficiary Amounts 2019 vs 2020')\n",
    "fig1.add_trace(\n",
    "    go.Scatter(x=main_df['Month'], y=main_df[main_df['Year']==2020].groupby(['Month'])['Benefit_Amount'].mean(),line=dict(color='firebrick', width=2), mode='lines',name=\"2020\"))\n",
    "fig1.add_trace(\n",
    "    go.Scatter(x=list(range(1,13)), y=vals,line=dict(color='royalblue', width=2), mode='lines',name=\"2019\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choropleth map showing the median household income(2018) by county\n",
    "\n",
    "fig = px.choropleth(main_df, geojson=counties, locations='County', color='Median_Income',\n",
    "                           range_color=(0, main_df['Median_Income'].max()),\n",
    "                           scope=\"usa\",featureidkey=\"properties.name\",\n",
    "                    \n",
    "                           labels={'Median_Income':'Median Income per County'}\n",
    "                          )\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Benefit amnount paid out in 2019\n",
    "\n",
    "coun = main_df['County'].unique().tolist()\n",
    "mean_amount = []\n",
    "for c in coun :\n",
    "    mean_amount.append(df_2019[df_2019['County']==c]['Benefit_Amount'].mean())\n",
    "dd = pd.DataFrame()\n",
    "dd['County']= coun\n",
    "dd['Benefit_Amount']=mean_amount\n",
    "\n",
    "fig = px.choropleth(dd, geojson=counties, locations='County', color='Benefit_Amount',\n",
    "                           range_color=(dd['Benefit_Amount'].min(), dd['Benefit_Amount'].max()),\n",
    "                           scope=\"usa\",featureidkey=\"properties.name\",\n",
    "                    \n",
    "                           labels={'Benefit_Amount':'Benefit Amount per County'}\n",
    "                          )\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Benefit amount paid out by county in 2020\n",
    "\n",
    "coun = main_df['County'].unique().tolist()\n",
    "mean_amount = []\n",
    "for c in coun :\n",
    "    mean_amount.append(df_2020[df_2020['County']==c]['Benefit_Amount'].mean())\n",
    "dd = pd.DataFrame()\n",
    "dd['County']= coun\n",
    "dd['Benefit_Amount']=mean_amount\n",
    "\n",
    "fig = px.choropleth(dd, geojson=counties, locations='County', color='Benefit_Amount',\n",
    "                           range_color=(dd['Benefit_Amount'].min(), dd['Benefit_Amount'].max()),\n",
    "                           scope=\"usa\",featureidkey=\"properties.name\",\n",
    "                    \n",
    "                           labels={'Benefit_Amount':'Benefit Amount per County'}\n",
    "                          )\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpr1 = main_df[main_df['Year']==2019].groupby(['Month'])['Benefit_Amount'].mean()\n",
    "vals = []\n",
    "for i in list(range(1,13)) :\n",
    "    vals.append(dfpr1[i])\n",
    "vals\n",
    "fig1 = go.Figure()\n",
    "fig1.update_layout(title_text = 'Mean Beneficiary Amounts 2019 vs 2020')\n",
    "fig1.add_trace(\n",
    "    go.Scatter(x=main_df['Month'], y=main_df[main_df['Year']==2020].groupby(['Month'])['Benefit_Amount'].mean(),line=dict(color='firebrick', width=2), mode='lines',name=\"2020\"))\n",
    "fig1.add_trace(\n",
    "    go.Scatter(x=list(range(1,13)), y=vals,line=dict(color='royalblue', width=2), mode='lines',name=\"2019\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficent Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## converting income to int\n",
    "inc = df_2020['Median_Income'].tolist()\n",
    "for i in range(len(inc)) :\n",
    "    inc[i]=inc[i].replace(',','')\n",
    "df_2020['Median_Income'] = inc\n",
    "df_2020['Median_Income'] = df_2020['Median_Income'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc = df_2019['Median_Income'].tolist()\n",
    "for i in range(len(inc)) :\n",
    "    inc[i]=inc[i].replace(',','')\n",
    "df_2019['Median_Income'] = inc\n",
    "df_2019['Median_Income'] = df_2019['Median_Income'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor = df_2020.corr()\n",
    "cor.style.background_gradient(cmap='BrBG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor2 = df_2019.corr()\n",
    "cor2.style.background_gradient(cmap = 'BrBG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Principle Component Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_2020[['Median_Income','Beneficiaries']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scaling the data for PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "x = StandardScaler().fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 2)\n",
    "principalComponents = pca.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "principalComponents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df_2020['Benefit_Amount']\n",
    "target.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## splitting the benefit amounts into 6 categories based on range of payments\n",
    "target.ravel()\n",
    "bins = pd.cut(target, 6, labels = ['very_low', 'low', 'medium', 'medium_high', 'high', 'very_high'])\n",
    "bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## combining scaled pc's with now categorical benefit_amount \n",
    "finalDf = pd.concat([principalDf, bins], axis = 1)\n",
    "finalDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting the PCA Analysis\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('2 component PCA', fontsize = 20)\n",
    "targets = ['very_low', 'low', 'medium', 'medium_high', 'high', 'very_high']\n",
    "colors = ['r', 'g', 'b','y', 'c', 'm' ]\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = finalDf['Benefit_Amount'] == target\n",
    "    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n",
    "               , finalDf.loc[indicesToKeep, 'principal component 2']\n",
    "               , c = color\n",
    "               , s = 50)\n",
    "ax.legend(targets)\n",
    "ax.grid(linestyle = '-', color = 'white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## component 1 describes 61% of varaince while PC2 explains ~ 39%\n",
    "## where component 1 is median_income and 2 is # beneeficiaries\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First, have to encode the variables, drop beneficiaries because of its high correlation with benefit_amount\n",
    "## Beneficiaries used later for comparison to model predictions\n",
    "features = pd.get_dummies(df_2020.drop('Beneficiaries', axis = 1))\n",
    "features.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## separating the benefit_amount\n",
    "labels = np.array(features['Benefit_Amount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features= features.drop('Benefit_Amount', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = list(features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## splitting to test and train\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Features Shape:', train_features.shape)\n",
    "print('Training Labels Shape:', train_labels.shape)\n",
    "print('Testing Features Shape:', test_features.shape)\n",
    "print('Testing Labels Shape:', test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training the model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
    "rf.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## predicting the test set\n",
    "predictions = rf.predict(test_features)\n",
    "# Calculate the absolute errors\n",
    "errors = abs(predictions - test_labels)\n",
    "# Print out the mean absolute error\n",
    "print('Mean Absolute Error:', round(np.mean(errors), 2), 'dollars.', \"In comparison to the max 430,000,000 dollars.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Listing the variables and their relative importance to predicting benefit_amount\n",
    "importances = list(rf.feature_importances_)\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "[print('Variable: {:24} Importance: {}'.format(*pair)) for pair in feature_importances];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## getting the non-zero features and variables for visualizations\n",
    "fm = feature_importances[0:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = []\n",
    "for a,b in fm:\n",
    "    imp.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importance Plot For Benefit_Amount\n",
    "plt.style.use('ggplot')\n",
    "x_values = list(range(len(imp)))\n",
    "plt.bar(x_values, imp, orientation = 'vertical', color=(0.2,0.4,0.6))\n",
    "plt.xticks(x_values,fm, rotation='vertical')\n",
    "plt.ylabel('Importance'); plt.xlabel('Variable'); plt.title('Variable Importances')\n",
    "plt.figure(figsize = (8,8))\n",
    "plt.tight_layout()\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_statement = '''select year,\n",
    "                    sum(Benefit_Amount) as Benifit_Amount\n",
    "                    from Unemployment \n",
    "                    group by year\n",
    "                    limit 20;'''\n",
    "df = pd.read_sql_query(sql_statement, conn)\n",
    "#display(df)\n",
    "\n",
    "x = df['Year']\n",
    "y = df['Benifit_Amount']\n",
    "plt.plot(x,y)\n",
    "plt.ylabel('Benifit Amount')\n",
    "plt.xlabel(' Year')\n",
    "#plt.legend([''])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_statement = '''select month,\n",
    "                    sum(Beneficiaries) as Beneficiaries\n",
    "                    from unemployment \n",
    "                    where year = 2020\n",
    "                    group by month  ;'''\n",
    "\n",
    "sql_statement1 = '''select month,\n",
    "                    sum(Beneficiaries) as Beneficiaries\n",
    "                    from unemployment \n",
    "                    where year = 2019\n",
    "                    group by month  ;'''\n",
    "sql_statement2 = '''select month,\n",
    "                    sum(Beneficiaries) as Beneficiaries\n",
    "                    from unemployment \n",
    "                    where year = 2009\n",
    "                    group by month  ;'''\n",
    "sql_statement3 = '''select month,\n",
    "                    sum(Beneficiaries) as Beneficiaries\n",
    "                    from unemployment \n",
    "                    where year = 2008\n",
    "                    group by month  ;'''\n",
    "df_2020 = pd.read_sql_query(sql_statement, conn)\n",
    "df_2019 = pd.read_sql_query(sql_statement1, conn)\n",
    "df_2008 = pd.read_sql_query(sql_statement2, conn)\n",
    "df_2009 = pd.read_sql_query(sql_statement3, conn)\n",
    "\n",
    "plt.plot(df_2020['Month'],df_2020['Beneficiaries'])\n",
    "plt.plot(df_2019['Month'],df_2019['Beneficiaries'])\n",
    "plt.plot(df_2008['Month'],df_2008['Beneficiaries'])\n",
    "plt.plot(df_2009['Month'],df_2009['Beneficiaries'])\n",
    "\n",
    "\n",
    "plt.ylabel('Unemployment payout claiming People')\n",
    "plt.xlabel(' Month')\n",
    "plt.legend([2020,2009,2008,2019])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from matplotlib.pylab import rcParams\n",
    "sql_statement = '''select Year||'-'|| month as Date,  \n",
    "                    sum(Beneficiaries) as Beneficiaries \n",
    "                    from unemployment  \n",
    "                     group by year,month \n",
    "                     order by year,month\n",
    "                     '''\n",
    "\n",
    "df = pd.read_sql_query(sql_statement, conn)\n",
    "df.drop(238, axis = 0, inplace = True)\n",
    "display(df.tail())\n",
    "\n",
    "df['Date'] = pd.to_datetime(df['Date'], infer_datetime_format= True)\n",
    "indexedDataset =df.set_index(['Date'])\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Number of Beneficiaries\")\n",
    "plt.plot(indexedDataset)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H0 Data is NON-Stationary\n",
    "# H1 Data is stationary\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "def adfuller_test(indexedDataset):\n",
    "    print('Results of Dickey-Fuller Test:')\n",
    "    dftest = adfuller(indexedDataset['Beneficiaries'], autolag = 'AIC')\n",
    "\n",
    "    dfoutput = pd.Series(dftest[0:4], index = ['Test Statistic', 'p-value', '#Lags Used','No of Observations Used'])\n",
    "    # for key,value in dftest[4].items():\n",
    "    #     dfoutput['Critical Value (%s)' %key] = value\n",
    "    display(dfoutput)\n",
    "    if(dfoutput[1] <= 0.05):\n",
    "        display('STRONG Evedience against Null hypotheis, reject Null hypothesis- DATA IS STATIONARY')\n",
    "    else:\n",
    "        display('WEAK Evedience against Null hypotheis,Time-series has a unit Root, Indicating -- DATA IS STATIONARY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform AD-Fuller test:\n",
    "adfuller_test(indexedDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetDiffShifting = indexedDataset - indexedDataset.shift(6)\n",
    "plt.plot(datasetDiffShifting)\n",
    "plt.show()\n",
    "#display(datasetDiffShifting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Again test dickey fuller test\n",
    "adfuller_test(datasetDiffShifting.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetDiffShifting.dropna(inplace = True)\n",
    "#ACF and PACF plots:\n",
    "\n",
    "from statsmodels.tsa.stattools import acf,pacf\n",
    "\n",
    "lag_acf = acf(datasetDiffShifting, nlags = 20)\n",
    "lag_pacf = pacf(datasetDiffShifting, nlags = 20, method = 'ols')\n",
    "\n",
    "\n",
    "#plot ACF\n",
    "plt.subplot(121)\n",
    "plt.plot(lag_acf)\n",
    "plt.axhline(y=0,linestyle = '--', color = 'gray')\n",
    "plt.axhline(y=-1.96/np.sqrt(len(datasetDiffShifting)),linestyle = '--', color = 'gray')\n",
    "plt.axhline(y=-1.96/np.sqrt(len(datasetDiffShifting)),linestyle = '--', color = 'gray')\n",
    "plt.title(\"Autocorrelation Functions\")\n",
    "\n",
    "#plot PACF\n",
    "plt.subplot(122)\n",
    "plt.plot(lag_pacf)\n",
    "plt.axhline(y=0,linestyle = '--', color = 'gray')\n",
    "plt.axhline(y=-1.96/np.sqrt(len(datasetLogDiffShifting)),linestyle = '--', color = 'gray')\n",
    "plt.axhline(y=-1.96/np.sqrt(len(datasetLogDiffShifting)),linestyle = '--', color = 'gray')\n",
    "plt.title(\"Autocorrelation Functions\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA MODEL\n",
    "model = ARIMA(indexedDataset, order=(2,1,2))\n",
    "results_ARIMA = model.fit(disp = -1)\n",
    "# plt.plot(datasetDiffShifting)\n",
    "# plt.plot(results_ARIMA.fittedvalues, color = 'red')\n",
    "# plt.title('RSS: %.4f'% sum(results_ARIMA.fittedvalues-datasetDiffShifting['Beneficiaries'])**2)\n",
    "# print('Plotting AR model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "results_ARIMA.plot_predict(1,298) \n",
    "\n",
    "#238 past rows + 60( next 5 years = 12months *5 years)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
